{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting lark\n",
      "  Downloading lark-1.1.7-py3-none-any.whl (108 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m108.9/108.9 kB\u001b[0m \u001b[31m717.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: lark\n",
      "Successfully installed lark-1.1.7\n"
     ]
    }
   ],
   "source": [
    "!pip install lark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "import sys\n",
    "sys.path.append('../..')\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.retrievers.self_query.base import SelfQueryRetriever\n",
    "from langchain.chains.query_constructor.base import AttributeInfo\n",
    "\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import LLMChainExtractor\n",
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import RetrievalQA, ConversationalRetrievalChain\n",
    "\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = load_dotenv(find_dotenv()) # read local .env file\n",
    "\n",
    "openai.api_key  = os.environ['OPENAI_API_KEY']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading of PDF assets\n",
    "\n",
    "The load of PDF documents is used to insert the data in the langflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "loader = PyPDFLoader(\"../inputs/Dissertação_inglês_10_09_23_pt2.pdf\")\n",
    "doc = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each page is a `Document`.\n",
    "\n",
    "A `Document` contains text (`page_content`) and `metadata`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "87"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acronyms list\n",
      "3GPP3rd Generation Partnership Project\n",
      "ACFAutoCorrelation Function\n",
      "AIArtificial Intelligence\n",
      "ARIMA AutoRegressive Moving Average\n",
      "BSBase Station\n",
      "BSsBase Stations\n",
      "CDRCall Detail Records\n",
      "CNCore Network\n",
      "DFTDiscrete Fourier Transform\n",
      "ETSIEuropean Telecommunications Standards Institute\n",
      "GDPR General Data Protection Regulation\n"
     ]
    }
   ],
   "source": [
    "page = doc[22]\n",
    "print(page.page_content[0:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'source': '../inputs/Dissertação_inglês_10_09_23_pt2.pdf', 'page': 22}\n"
     ]
    }
   ],
   "source": [
    "print(page.metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creation of a Chroma Vectorstore\n",
    "\n",
    "A vectorstore is a object that stores text data in a format that allows easy retrieval of information. This data is the context (text that stores information about the problem) embedded using a embedding. In this case, we are going to use OpenAI embedding.\n",
    "\n",
    "The first step is to **split** the document in batches that allows the insertion of the data in the context input of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 1500,\n",
    "    chunk_overlap = 150\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = text_splitter.split_documents(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "122"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it is time to make **embeddings** of the information using the embedding of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -Rf /Users/patrick/Documents/Pessoal/paper-rag/chroma/\n",
    "\n",
    "embedding = OpenAIEmbeddings()\n",
    "persist_directory = '/Users/patrick/Documents/Pessoal/paper-rag/chroma/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectordb = Chroma.from_documents(\n",
    "    documents=splits,\n",
    "    embedding=embedding,\n",
    "    persist_directory=persist_directory\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "122\n"
     ]
    }
   ],
   "source": [
    "print(vectordb._collection.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing some retrieval searches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usagesofNFVintheindustryand, accordingtoEuropeanTelecommunicationsStandards\n",
      "Institute (ETSI) (ETSI, 2013), some advantages of network virtualization that stand out\n",
      "are:\n",
      "1. NFV as a service: a NFV can be provided as a service by a network operator similar\n",
      "to cloud computing services (RANKOTHGE et al., 2015);\n",
      "2. Virtualization of Core Network (CN) and BSs (BASTA et al., 2014);\n",
      "3. Virtualization of the home environment: installation of new equipment and on-site\n",
      "technical support can be less frequent (BRONSTEIN; SHRAGA, 2014);\n"
     ]
    }
   ],
   "source": [
    "question = \"what means NFV?\"\n",
    "docs_ssearch = vectordb.similarity_search(question,k=3)  # Simple similarity search\n",
    "print(docs_ssearch[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usagesofNFVintheindustryand, accordingtoEuropeanTelecommunicationsStandards\n",
      "Institute (ETSI) (ETSI, 2013), some advantages of network virtualization that stand out\n",
      "are:\n",
      "1. NFV as a service: a NFV can be provided as a service by a network operator similar\n",
      "to cloud computing services (RANKOTHGE et al., 2015);\n",
      "2. Virtualization of Core Network (CN) and BSs (BASTA et al., 2014);\n",
      "3. Virtualization of the home environment: installation of new equipment and on-site\n",
      "technical support can be less frequent (BRONSTEIN; SHRAGA, 2014);\n"
     ]
    }
   ],
   "source": [
    "doc_mmr = vectordb.max_marginal_relevance_search(question,k=2, fetch_k=3)  # MMR is a technique that tries to get \n",
    "print(doc_mmr[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing metadata self-query retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_field_info = [\n",
    "    AttributeInfo(\n",
    "        name=\"page\",\n",
    "        description=\"The page from the lecture\",\n",
    "        type=\"integer\",\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_content_description = \"Lecture notes\"\n",
    "llm = OpenAI(temperature=0)\n",
    "retriever = SelfQueryRetriever.from_llm(\n",
    "    llm=llm,\n",
    "    vectorstore=vectordb,\n",
    "    document_contents=document_content_description,\n",
    "    metadata_field_info=metadata_field_info,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"what means NFV?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/patrick/opt/anaconda3/envs/langflow/lib/python3.11/site-packages/langchain/chains/llm.py:275: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query='NFV' filter=None limit=None\n",
      "{'source': '../inputs/Dissertação_inglês_10_09_23_pt2.pdf', 'page': 53}\n",
      "usagesofNFVintheindustryand, accordingtoEuropeanTelecommunicationsStandards\n",
      "Institute (ETSI) (ETSI, 2013), some advantages of network virtualization that stand out\n",
      "are:\n",
      "1. NFV as a service: a NFV can be provided as a service by a network operator similar\n",
      "to cloud computing services (RANKOTHGE et al., 2015);\n",
      "2. Virtualization of Core Network (CN) and BSs (BASTA et al., 2014);\n",
      "3. Virtualization of the home environment: installation of new equipment and on-site\n",
      "technical support can be less frequent (BRONSTEIN; SHRAGA, 2014);\n",
      "\n",
      "\n",
      "{'source': '../inputs/Dissertação_inglês_10_09_23_pt2.pdf', 'page': 54}\n",
      "4.2. The predictive model in the 5G infrastructure 41\n",
      "4. Virtualization of CDNs (MANGILI; MARTIGNON; CAPONE, 2014; KIM; LEE,\n",
      "2014).\n",
      "The main barrier of this new approach is the overall performance, especially in middle-\n",
      "boxchains. Sometrafficcouldflowthroughvariousmiddleboxesbasedontheirneeds, e.g.,\n",
      "a proxy request that need to pass through firewall, Intrusion Detection Systems (IDSs)\n",
      "and the proxy service itself. However, the literature shows that the NFV can achieve\n",
      "almost a hardware-based performance (MARTINS et al., 2014; HWANG, 2014).\n",
      "Based on the NFV architecture, middleboxes could rely in different local servers or\n",
      "even remote ones in the cloud, as far as they could achieve the QoS and QoE require-\n",
      "ments and can also have their resources dynamically optimized, mainly oriented to save\n",
      "financial and computational resources. This new approach allows also that conventional\n",
      "middleboxes could be used with cloud as well as general purpose local hardware, allowing\n",
      "a mix architecture to attend many necessities. An architectural example can be seen in\n",
      "Figure 14.\n",
      "Figure 14 – Implementation option of NFV architecture to the network in Figure 13.\n",
      "Source: the authors.\n",
      "The optimization of NFV services are done by the orchestrator, which find the optimal\n",
      "scenario to the network services based on service metrics and also on Network Traffic\n",
      "Monitoring and Analysis (NTMA). NTMA are VNFs that, based on historical data,\n",
      "\n",
      "\n",
      "{'source': '../inputs/Dissertação_inglês_10_09_23_pt2.pdf', 'page': 82}\n",
      "BARI, F. et al. Orchestrating virtualized network functions. IEEE Transactions on\n",
      "Network and Service Management , v. 13, n. 4, p. 725–739, 2016.\n",
      "Barlacchi, G. et al. A multi-source dataset of urban life in the city of milan and the\n",
      "province of trentino. Sci Data 2 , 2015.\n",
      "BASTA, A. et al. Applying nfv and sdn to lte mobile core gateways, the functions\n",
      "placement problem. In: Proceedings of the 4th workshop on All things cellular:\n",
      "operations, applications, & challenges . [S.l.: s.n.], 2014. p. 33–38.\n",
      "BOUTABA, R. et al. A comprehensive survey on machine learning for networking:\n",
      "evolution, applications and research opportunities. Journal of Internet Services and\n",
      "Applications , Springer, v. 9, n. 1, p. 1–99, 2018.\n",
      "\n",
      "\n",
      "{'source': '../inputs/Dissertação_inglês_10_09_23_pt2.pdf', 'page': 53}\n",
      "40 Chapter 4. Preliminaries on data collection for MTP-NT\n",
      "4.2 The predictive model in the 5G infrastructure\n",
      "Inpreviousnetworkinfrastructures, somespecificfunctionswereperformedbymiddle-\n",
      "boxes, hardware-based applications as firewall, Intrusion Detection System (IDS), proxy,\n",
      "encryption, data monitoring and other services. These services were usually deployed\n",
      "on proprietary hardware, placed at fixed locations and needed specialized personal for\n",
      "deployment and maintenance (BARI et al., 2016), as seen in Figure 13.\n",
      "Figure 13 – Conventional network based on proprietary hardware architecture. Source:\n",
      "the authors.\n",
      "These middleboxes are static hardware that perform single tasks, not allowing new\n",
      "functionality and subjecting the telecommunications operator to short deployment and\n",
      "replacement cycles to keep up with new demands and technologies. A suitable option to\n",
      "improve this architecture, as seen in new networks like 5G, is the use of NFV, an approach\n",
      "where those services provided in the network become software based middleboxes, called\n",
      "Virtual Network Functions (VNF), typically as Virtual Machines or Containers. Instead\n",
      "of relying on proprietary hardware, these middleboxes are running on both cloud and\n",
      "local general purpose hardware such as high volume servers and could be dynamically\n",
      "provisioned based on the network needs.\n",
      "(HERRERA; BOTERO, 2016) make a brief explanation of the main advantages and\n",
      "usagesofNFVintheindustryand, accordingtoEuropeanTelecommunicationsStandards\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "docs = retriever.get_relevant_documents(question)\n",
    "\n",
    "for d in docs:\n",
    "    print(d.metadata)\n",
    "    print(d.page_content)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final solution: compression method\n",
    "\n",
    "Ref: https://blog.langchain.dev/improving-document-retrieval-with-contextual-compression/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrap our vectorstore\n",
    "llm = OpenAI(temperature=0)  # Model used\n",
    "compressor = LLMChainExtractor.from_llm(llm)  # uses an LLMChain to extract from each document only the statements that are relevant to the query.\n",
    "\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor,\n",
    "    base_retriever=vectordb.as_retriever(search_type = \"mmr\")  # Retriever method using MMR retriavel to ensure variability in the answer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretty_print_docs(docs):\n",
    "    print(f\"\\n{'-' * 100}\\n\".join([f\"Document {i+1}:\\n\\n\" + d.page_content for i, d in enumerate(docs)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 1:\n",
      "\n",
      "Milan, Trento, Telecom-munications dataset in Milan, Novem-ber 1st, 2013 and December 31st, 2013, 10,000zonal regions listed in the city\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 2:\n",
      "\n",
      "\"The optimizer used was Adamax, which is based on Adam (KINGMA; BA, 2017). The loss function was the Mean Squared Error (MSE) and 80% of the entire dataset was used to train, with the other 20% used as test.\"\n"
     ]
    }
   ],
   "source": [
    "question = \"Is the model trained on Milan or Trento data?\"\n",
    "compressed_docs = compression_retriever.get_relevant_documents(question)\n",
    "pretty_print_docs(compressed_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making a Q&A chatbot to talk with PDF data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build prompt\n",
    "template = \"\"\"Use the following pieces of context to answer the question at the end. If you can't make a answer with context, just say that you don't know, don't try to make up an answer. \n",
    "{context}\n",
    "Question: {question}\n",
    "Helpful Answer:\"\"\"\n",
    "\n",
    "QA_CHAIN_PROMPT = PromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run chain\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm,\n",
    "    retriever=compression_retriever,\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/patrick/opt/anaconda3/envs/langflow/lib/python3.11/site-packages/langchain/chains/llm.py:275: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " It depends on the specific needs of the project. Each of the databases mentioned (Cassandra, HBase, and Redis) have different strengths and weaknesses, so it is important to consider the specific needs of the project before deciding which one to use.\n"
     ]
    }
   ],
   "source": [
    "question = \"which database is recommended to be used and why\"\n",
    "result = qa_chain({\"query\": question})\n",
    "print(result[\"result\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationBufferMemory(\n",
    "    memory_key=\"chat_history\",\n",
    "    return_messages=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for ConversationalRetrievalChain\nchain_type_kwargs\n  extra fields not permitted (type=value_error.extra)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m/Users/patrick/Documents/Pessoal/paper-rag/code/rag_chatgpt.ipynb Célula 36\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/patrick/Documents/Pessoal/paper-rag/code/rag_chatgpt.ipynb#X54sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Run chain\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/patrick/Documents/Pessoal/paper-rag/code/rag_chatgpt.ipynb#X54sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m qa \u001b[39m=\u001b[39m ConversationalRetrievalChain\u001b[39m.\u001b[39;49mfrom_llm(\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/patrick/Documents/Pessoal/paper-rag/code/rag_chatgpt.ipynb#X54sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     llm,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/patrick/Documents/Pessoal/paper-rag/code/rag_chatgpt.ipynb#X54sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     retriever\u001b[39m=\u001b[39;49mcompression_retriever,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/patrick/Documents/Pessoal/paper-rag/code/rag_chatgpt.ipynb#X54sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     \u001b[39m# retriever=vectordb.as_retriever(),\u001b[39;49;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/patrick/Documents/Pessoal/paper-rag/code/rag_chatgpt.ipynb#X54sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     memory\u001b[39m=\u001b[39;49mmemory,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/patrick/Documents/Pessoal/paper-rag/code/rag_chatgpt.ipynb#X54sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     return_source_documents\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/patrick/Documents/Pessoal/paper-rag/code/rag_chatgpt.ipynb#X54sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     chain_type_kwargs\u001b[39m=\u001b[39;49m{\u001b[39m\"\u001b[39;49m\u001b[39mprompt\u001b[39;49m\u001b[39m\"\u001b[39;49m: QA_CHAIN_PROMPT}\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/patrick/Documents/Pessoal/paper-rag/code/rag_chatgpt.ipynb#X54sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m )\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/langflow/lib/python3.11/site-packages/langchain/chains/conversational_retrieval/base.py:228\u001b[0m, in \u001b[0;36mConversationalRetrievalChain.from_llm\u001b[0;34m(cls, llm, retriever, condense_question_prompt, chain_type, verbose, condense_question_llm, combine_docs_chain_kwargs, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    221\u001b[0m _llm \u001b[39m=\u001b[39m condense_question_llm \u001b[39mor\u001b[39;00m llm\n\u001b[1;32m    222\u001b[0m condense_question_chain \u001b[39m=\u001b[39m LLMChain(\n\u001b[1;32m    223\u001b[0m     llm\u001b[39m=\u001b[39m_llm,\n\u001b[1;32m    224\u001b[0m     prompt\u001b[39m=\u001b[39mcondense_question_prompt,\n\u001b[1;32m    225\u001b[0m     verbose\u001b[39m=\u001b[39mverbose,\n\u001b[1;32m    226\u001b[0m     callbacks\u001b[39m=\u001b[39mcallbacks,\n\u001b[1;32m    227\u001b[0m )\n\u001b[0;32m--> 228\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39;49m(\n\u001b[1;32m    229\u001b[0m     retriever\u001b[39m=\u001b[39;49mretriever,\n\u001b[1;32m    230\u001b[0m     combine_docs_chain\u001b[39m=\u001b[39;49mdoc_chain,\n\u001b[1;32m    231\u001b[0m     question_generator\u001b[39m=\u001b[39;49mcondense_question_chain,\n\u001b[1;32m    232\u001b[0m     callbacks\u001b[39m=\u001b[39;49mcallbacks,\n\u001b[1;32m    233\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    234\u001b[0m )\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/langflow/lib/python3.11/site-packages/langchain/load/serializable.py:74\u001b[0m, in \u001b[0;36mSerializable.__init__\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m---> 74\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     75\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lc_kwargs \u001b[39m=\u001b[39m kwargs\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/langflow/lib/python3.11/site-packages/pydantic/main.py:341\u001b[0m, in \u001b[0;36mpydantic.main.BaseModel.__init__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValidationError\u001b[0m: 1 validation error for ConversationalRetrievalChain\nchain_type_kwargs\n  extra fields not permitted (type=value_error.extra)"
     ]
    }
   ],
   "source": [
    "# Run chain\n",
    "qa = ConversationalRetrievalChain.from_llm(\n",
    "    llm,\n",
    "    retriever=compression_retriever,\n",
    "    # retriever=vectordb.as_retriever(),\n",
    "    memory=memory,\n",
    "    return_source_documents=False,\n",
    "    chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The author does not recommend any specific databases, they just mention that the database used is publicly available.\n"
     ]
    }
   ],
   "source": [
    "question = \"Which databases are recommended by the author?\"\n",
    "result = qa({\"question\": question})\n",
    "print(result['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " I don't know.\n"
     ]
    }
   ],
   "source": [
    "question = \"Which is the publicly available database mentioned by the author?\"\n",
    "result = qa({\"question\": question})\n",
    "print(result['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
